# Intro
This repo contains a CUDA implementation of the SymSum activation function as a [PyTorch extention](https://pytorch.org/tutorials/advanced/cpp_extension.html).  
The implementation leverages the optimization described in the paper "Say Goodbye to Gradient Vanishing".  
It achieves the same speed performance as Pytorch's built-in ReLU function. 

# Installation

# Usage
